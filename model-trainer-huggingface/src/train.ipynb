{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c3e4532",
   "metadata": {},
   "source": [
    "# Train models using HuggingFace libraries\n",
    "\n",
    "This notebook takes parameters from a params.json file which is automatically\n",
    "created by Substratus K8s operator.\n",
    "\n",
    "The following parameters influence what happens in this notebook:\n",
    "- `dataset_urls`: A comma separated list of URLs. The URLs should point to\n",
    "  json files that contain your training dataset. If unset a json or jsonl\n",
    "  file should be present under the `/content/data/` directory.\n",
    "- `prompt_template`: The prompt template to use for training\n",
    "- `push_to_hub`: if this variable is set a repo id, then the trained\n",
    "  model will get pushed to HuggingFace hub. For example,\n",
    "  set it to \"substratusai/my-model\" to publish to substratusai HF org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86ccd646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_urls': 'https://huggingface.co/datasets/weaviate/WithRetrieval-SchemaSplit-Train-80/resolve/main/WithRetrieval-Random-Train-80.json',\n",
       " 'inference_prompt_template': '## Instruction\\nYour task is to write GraphQL for the Natural Language Query provided. Use the provided API reference and Schema to generate the GraphQL. The GraphQL should be valid for Weaviate.\\n\\nOnly use the API reference to understand the syntax of the request.\\n\\n## Natural Language Query\\n{nlcommand}\\n\\n## Schema\\n{schema}\\n\\n## API reference\\n{apiRef}\\n\\n## Answer\\n```graphql\\n',\n",
       " 'logging_steps': 50,\n",
       " 'num_train_epochs': 3,\n",
       " 'prompt_template': '## Instruction\\nYour task is to write GraphQL for the Natural Language Query provided. Use the provided API reference and Schema to generate the GraphQL. The GraphQL should be valid for Weaviate.\\n\\nOnly use the API reference to understand the syntax of the request.\\n\\n## Natural Language Query\\n{nlcommand}\\n\\n## Schema\\n{schema}\\n\\n## API reference\\n{apiRef}\\n\\n## Answer\\n{output}\\n',\n",
       " 'push_to_hub': 'substratusai/wgql-WithRetrieval-SchemaSplit-Train-80',\n",
       " 'save_steps': 50,\n",
       " 'target_modules\"': 'q_proj, up_proj, o_proj, k_proj, down_proj, gate_proj, v_proj',\n",
       " 'warmup_steps': 100,\n",
       " 'target_modules': 'q_proj, up_proj, o_proj, k_proj, down_proj, gate_proj, v_proj'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "params = {}\n",
    "params_path = Path(\"/content/params.json\")\n",
    "if params_path.is_file():\n",
    "    with params_path.open(\"r\", encoding=\"UTF-8\") as params_file:\n",
    "        params = json.load(params_file)\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fafd16b-d8c9-47bf-9116-c27b1d43a019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the following URLs for the dataset: ['https://huggingface.co/datasets/weaviate/WithRetrieval-SchemaSplit-Train-80/resolve/main/WithRetrieval-Random-Train-80.json']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'output', 'nlcommand', 'apiRef', 'apiRefPath', 'schema', 'schemaPath'],\n",
       "        num_rows: 3190\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset_urls = params.get(\"dataset_urls\")\n",
    "if dataset_urls:\n",
    "    urls = [u.strip() for u in dataset_urls.split(\",\")]\n",
    "    print(f\"Using the following URLs for the dataset: {urls}\")\n",
    "    data = load_dataset(\"json\", data_files=urls)\n",
    "else:\n",
    "    data = load_dataset(\"json\", data_files=\"/content/data/*.json*\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08e478fa-d095-4145-9bd1-b4feec7bc4f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b817d2fecfd74276898eb86053905538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import sys\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_path = \"/content/model/\"\n",
    "trained_model_path = \"/content/artifacts\"\n",
    "trained_model_path_lora = \"/content/artifacts/lora\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path,\n",
    "                                          local_files_only=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path, torch_dtype=torch.float16, device_map=\"auto\", trust_remote_code=True, load_in_8bit=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88908150-1585-4781-9542-d68193d808bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_name_or_path\": \"/content/model/\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 11008,\n",
       "  \"max_position_embeddings\": 4096,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 32,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"quantization_config\": {\n",
       "    \"bnb_4bit_compute_dtype\": \"float32\",\n",
       "    \"bnb_4bit_quant_type\": \"fp4\",\n",
       "    \"bnb_4bit_use_double_quant\": false,\n",
       "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
       "    \"llm_int8_has_fp16_weight\": false,\n",
       "    \"llm_int8_skip_modules\": null,\n",
       "    \"llm_int8_threshold\": 6.0,\n",
       "    \"load_in_4bit\": false,\n",
       "    \"load_in_8bit\": true,\n",
       "    \"quant_method\": \"bitsandbytes\"\n",
       "  },\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.34.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec8a1a9f-fe60-49c7-ab20-04034323df8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Instruction\n",
      "Your task is to write GraphQL for the Natural Language Query provided. Use the provided API reference and Schema to generate the GraphQL. The GraphQL should be valid for Weaviate.\n",
      "\n",
      "Only use the API reference to understand the syntax of the request.\n",
      "\n",
      "## Natural Language Query\n",
      "{nlcommand}\n",
      "\n",
      "## Schema\n",
      "{schema}\n",
      "\n",
      "## API reference\n",
      "{apiRef}\n",
      "\n",
      "## Answer\n",
      "{output}\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "default_prompt = \"\"\"\n",
    "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "{prompt}\n",
    "### Response:\n",
    "{completion}\n",
    "\"\"\"\n",
    "\n",
    "prompt = params.get(\"prompt_template\", default_prompt)\n",
    "\n",
    "eos_token = tokenizer.convert_ids_to_tokens(model.config.eos_token_id)\n",
    "if prompt[-len(eos_token):] != eos_token:\n",
    "    prompt = prompt + eos_token\n",
    "\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0abf96e1-3bc1-4ae7-80ac-c2e585e9c7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 19 06:34:18 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA L4           Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   70C    P0    36W /  72W |   7818MiB / 23034MiB |      3%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d1e1795-c783-4ddf-999e-f1de19258928",
   "metadata": {},
   "source": [
    "Prompt before fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5dd944b-e2bd-4bfd-a5fa-55bc90239926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='/content/model/', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32000: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict\n",
    "# source: https://github.com/artidoro/qlora\n",
    "DEFAULT_PAD_TOKEN = params.get(\"pad_token\", \"[PAD]\")\n",
    "\n",
    "def smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict: Dict,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    model: transformers.PreTrainedModel,\n",
    "):\n",
    "    \"\"\"Resize tokenizer and embedding.\n",
    "\n",
    "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
    "    \"\"\"\n",
    "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    if num_new_tokens > 0:\n",
    "        input_embeddings_data = model.get_input_embeddings().weight.data\n",
    "        output_embeddings_data = model.get_output_embeddings().weight.data\n",
    "\n",
    "        input_embeddings_avg = input_embeddings_data[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "        output_embeddings_avg = output_embeddings_data[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "\n",
    "        input_embeddings_data[-num_new_tokens:] = input_embeddings_avg\n",
    "        output_embeddings_data[-num_new_tokens:] = output_embeddings_avg\n",
    "\n",
    "if tokenizer._pad_token is None:\n",
    "    smart_tokenizer_and_embedding_resize(\n",
    "        special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "if isinstance(tokenizer, transformers.LlamaTokenizer):\n",
    "    # LLaMA tokenizer may not have correct special tokens set.\n",
    "    # Check and add them if missing to prevent them from being parsed into different tokens.\n",
    "    # Note that these are present in the vocabulary.\n",
    "    # Note also that `model.config.pad_token_id` is 0 which corresponds to `<unk>` token.\n",
    "    print('Adding special tokens.')\n",
    "    tokenizer.add_special_tokens({\n",
    "            \"eos_token\": tokenizer.convert_ids_to_tokens(model.config.eos_token_id),\n",
    "            \"bos_token\": tokenizer.convert_ids_to_tokens(model.config.bos_token_id),\n",
    "            \"unk_token\": tokenizer.convert_ids_to_tokens(\n",
    "                model.config.pad_token_id if model.config.pad_token_id != -1 else tokenizer.pad_token_id\n",
    "            ),\n",
    "    })\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e78b510d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After tokenizing: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'output', 'nlcommand', 'apiRef', 'apiRefPath', 'schema', 'schemaPath', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 3190\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "\n",
    "data = data.map(lambda x: tokenizer(prompt.format_map(x)))\n",
    "\n",
    "print(\"After tokenizing:\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dae6c6f-3ae1-4697-852e-fce24a82b9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_modules: q_proj, up_proj, o_proj, k_proj, down_proj, gate_proj, v_proj\n",
      "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=16, target_modules=['q_proj', 'up_proj', 'o_proj', 'k_proj', 'down_proj', 'gate_proj', 'v_proj'], lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None)\n",
      "trainable params: 39,976,960 || all params: 6,778,400,768 || trainable%: 0.5897697903718874\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\n",
    "\n",
    "target_modules = params.get(\"target_modules\")\n",
    "if target_modules:\n",
    "    target_modules = [mod.strip() for mod in target_modules.split(\",\")]\n",
    "\n",
    "lora_config2 = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=target_modules\n",
    ")\n",
    "print(lora_config2)\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# add LoRA adaptor\n",
    "model = get_peft_model(model, lora_config2)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70a3e36c-62cf-45aa-8f37-0db0e40857dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=1,\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_pin_memory=True,\n",
       "ddp_backend=None,\n",
       "ddp_broadcast_buffers=None,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "dispatch_batches=None,\n",
       "do_eval=False,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_steps=None,\n",
       "evaluation_strategy=no,\n",
       "fp16=True,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=4,\n",
       "gradient_checkpointing=False,\n",
       "greater_is_better=None,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_always_push=False,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=False,\n",
       "hub_strategy=every_save,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_inputs_for_metrics=False,\n",
       "include_tokens_per_second=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=3e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=False,\n",
       "local_rank=0,\n",
       "log_level=passive,\n",
       "log_level_replica=warning,\n",
       "log_on_each_node=True,\n",
       "logging_dir=/content/artifacts/checkpoints/runs/Oct19_06-35-12_wgqlg-withretrieval-schemasplit-train-80-v2-model-notebook,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=50,\n",
       "logging_strategy=steps,\n",
       "lr_scheduler_type=cosine,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=None,\n",
       "mp_parameters=,\n",
       "no_cuda=False,\n",
       "num_train_epochs=3.0,\n",
       "optim=paged_adamw_32bit,\n",
       "optim_args=None,\n",
       "output_dir=/content/artifacts/checkpoints,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=1,\n",
       "per_device_train_batch_size=1,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=[],\n",
       "resume_from_checkpoint=None,\n",
       "run_name=/content/artifacts/checkpoints,\n",
       "save_on_each_node=False,\n",
       "save_safetensors=False,\n",
       "save_steps=50,\n",
       "save_strategy=steps,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "sharded_ddp=[],\n",
       "skip_memory_metrics=True,\n",
       "tf32=None,\n",
       "torch_compile=False,\n",
       "torch_compile_backend=None,\n",
       "torch_compile_mode=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_cpu=False,\n",
       "use_ipex=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_mps_device=False,\n",
       "warmup_ratio=0.02,\n",
       "warmup_steps=100,\n",
       "weight_decay=0.0,\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import parse_training_args\n",
    "\n",
    "training_args = parse_training_args(params)\n",
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ae3e5f9-e28e-457b-b6bf-a62a472241bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'output', 'nlcommand', 'apiRef', 'apiRefPath', 'schema', 'schemaPath', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2871\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'output', 'nlcommand', 'apiRef', 'apiRefPath', 'schema', 'schemaPath', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 319\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[\"train\"].train_test_split(test_size=0.1)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33e407a-9d4f-49f6-a74b-b80db8cc3a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2151' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   2/2151 : < :, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=data[\"train\"],\n",
    "    eval_dataset=data[\"test\"],\n",
    "    args=training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "\n",
    "checkpoint_path = Path(\"/content/artifacts/checkpoints\")\n",
    "\n",
    "# Only set resume_from_checkpoint True when directory exists and contains files\n",
    "resume_from_checkpoint = checkpoint_path.is_dir() and any(checkpoint_path.iterdir())\n",
    "if resume_from_checkpoint:\n",
    "    print(\"Resuming from checkpoint:\", list(checkpoint_path.rglob(\"\")))\n",
    "trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "\n",
    "trainer.save_model(trained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea4e68e-57a7-48bd-bad9-f03dfe3f8a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f5dc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -lash {trained_model_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04651d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_prompt = params.get(\"inference_prompt_template\")\n",
    "if inference_prompt:\n",
    "    model.config.use_cache = True\n",
    "    device = \"cuda\"\n",
    "    model_inputs = tokenizer([inference_prompt.format_map(data[\"train\"][0])],\n",
    "                             return_tensors=\"pt\").to(device)\n",
    "\n",
    "    generated_ids = model.generate(**model_inputs,\n",
    "                                   max_new_tokens=300)\n",
    "\n",
    "    print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3837a68d-6a98-494e-a890-1135509c546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p {trained_model_path_lora}\n",
    "model.save_pretrained(trained_model_path_lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09db36b7-ead6-4368-9bfb-13ba1ba800a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "print(\"Clearing existing GPU memory to merge lora with base model\")\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Note this needs to happen in 16 bit hence the reload of the model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path, torch_dtype=torch.float16, device_map=\"auto\", trust_remote_code=True)\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, trained_model_path_lora, torch_dtype=torch.float16)\n",
    "model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260e9d79-6eb8-4516-bf8f-825a25606391",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(trained_model_path)\n",
    "tokenizer.save_pretrained(trained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d90a920-fb22-4291-8466-411ff41e31be",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -lash {trained_model_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29b870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running inference after merging lora layer\")\n",
    "inference_prompt = params.get(\"inference_prompt_template\")\n",
    "if inference_prompt:\n",
    "    model.config.use_cache = True\n",
    "    device = \"cuda\"\n",
    "    model_inputs = tokenizer([inference_prompt.format_map(data[\"train\"][0])],\n",
    "                             return_tensors=\"pt\").to(device)\n",
    "\n",
    "    generated_ids = model.generate(**model_inputs,\n",
    "                                   max_new_tokens=300)\n",
    "\n",
    "    print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202a694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import shutil\n",
    "\n",
    "tokenizer_model_path_base = Path(model_path) / \"tokenizer.model\"\n",
    "tokenizer_model_path_trained = Path(trained_model_path) / \"tokenizer.model\"\n",
    "if tokenizer_model_path_base.exists() and not tokenizer_model_path_trained.exists():\n",
    "    shutil.copy(tokenizer_model_path_base, tokenizer_model_path_trained)\n",
    "\n",
    "repo_id = params.get(\"push_to_hub\")\n",
    "if repo_id:\n",
    "    model.push_to_hub(repo_id)\n",
    "    tokenizer.push_to_hub(repo_id)\n",
    "    hf_api = HfApi()\n",
    "    # Upload tokenizer.model if it was in base model\n",
    "    if tokenizer_model_path_base.exists():\n",
    "        hf_api.upload_file(\n",
    "            path_or_fileobj=tokenizer_model_path_base,\n",
    "            path_in_repo=tokenizer_model_path_base.name,\n",
    "            repo_id=repo_id,\n",
    "        )\n",
    "    logs_path = Path(\"/content/artifacts/src/train.ipynb\")\n",
    "    if logs_path.exists():\n",
    "        hf_api.upload_file(\n",
    "            path_or_fileobj=logs_path,\n",
    "            path_in_repo=logs_path.name,\n",
    "            repo_id=repo_id,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccc5670",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
