{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfe44836-9ed7-4e33-8905-a096b79910e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "model_id = os.environ.get(\"PARAM_NAME\")\n",
    "if not model_id:\n",
    "    raise ValueError(\"Missing required environment variable PARAM_NAME. Set `params: {name: hf_org/model_id} in the model spec` \")\n",
    "\n",
    "output_dir = os.environ.get(\"OUTPUT_DIR\", \"/content/model\")\n",
    "\n",
    "# snapshot_download(repo_id=model_id, local_dir=output_dir, local_dir_use_symlinks=False, revision=\"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be15516c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['.gitattributes',\n",
       " 'README.md',\n",
       " 'config.json',\n",
       " 'configuration_RW.py',\n",
       " 'coreml/text-generation/falcon-7b-64-float32.mlpackage/Data/com.apple.CoreML/model.mlmodel',\n",
       " 'coreml/text-generation/falcon-7b-64-float32.mlpackage/Data/com.apple.CoreML/weights/weight.bin',\n",
       " 'coreml/text-generation/falcon-7b-64-float32.mlpackage/Manifest.json',\n",
       " 'generation_config.json',\n",
       " 'handler.py',\n",
       " 'modelling_RW.py',\n",
       " 'pytorch_model-00001-of-00002.bin',\n",
       " 'pytorch_model-00002-of-00002.bin',\n",
       " 'pytorch_model.bin.index.json',\n",
       " 'special_tokens_map.json',\n",
       " 'tokenizer.json',\n",
       " 'tokenizer_config.json']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub.hf_api import model_info\n",
    "from utils import filter_files\n",
    "\n",
    "model = model_info(model_id)\n",
    "\n",
    "files = os.environ.get(\"PARAM_FILES\", \"\")\n",
    "if files:\n",
    "    filenames = [f.strip() for f in files.split(\",\")]\n",
    "else:\n",
    "    filenames = [f.rfilename for f in model.siblings ]\n",
    "    filenames = filter_files(filenames)\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cc3beac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading .gitattributes to /content/model/.gitattributes\n",
      "Downloading README.md to /content/model/README.md\n",
      "Downloading config.json to /content/model/config.json\n",
      "Downloading configuration_RW.py to /content/model/configuration_RW.py\n",
      "Downloading generation_config.json to /content/model/generation_config.json\n",
      "Downloading handler.py to /content/model/handler.py\n",
      "Downloading modelling_RW.py to /content/model/modelling_RW.py\n",
      "Downloading pytorch_model-00001-of-00002.bin to /content/model/pytorch_model-00001-of-00002.bin\n",
      "Downloading pytorch_model-00002-of-00002.bin to /content/model/pytorch_model-00002-of-00002.bin\n",
      "Downloading pytorch_model.bin.index.json to /content/model/pytorch_model.bin.index.json\n",
      "Downloading special_tokens_map.json to /content/model/special_tokens_map.json\n",
      "Downloading tokenizer.json to /content/model/tokenizer.json\n",
      "Downloading tokenizer_config.json to /content/model/tokenizer_config.json\n",
      "Finished downloading /content/model/pytorch_model-00002-of-00002.bin\n",
      "Finished downloading /content/model/.gitattributes\n",
      "Finished downloading /content/model/special_tokens_map.json\n",
      "Finished downloading /content/model/pytorch_model.bin.index.json\n",
      "Finished downloading /content/model/handler.py\n",
      "Finished downloading /content/model/README.md\n",
      "Finished downloading /content/model/generation_config.json\n",
      "Finished downloading /content/model/tokenizer.json\n",
      "Finished downloading /content/model/configuration_RW.py\n",
      "Finished downloading /content/model/config.json\n",
      "Finished downloading /content/model/tokenizer_config.json\n",
      "Finished downloading /content/model/pytorch_model-00001-of-00002.bin\n",
      "Finished downloading /content/model/modelling_RW.py\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from pathlib import Path\n",
    "from huggingface_hub import hf_hub_url\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "token = os.getenv(\"HUGGING_FACE_HUB_TOKEN\")\n",
    "if token:\n",
    "    opener = urllib.request.build_opener()\n",
    "    opener.addheaders = [('authorization', f\"Bearer {token}\")]\n",
    "    urllib.request.install_opener(opener)\n",
    "\n",
    "def download_file(filename: str) -> str:\n",
    "    destination = f\"{output_dir}/{filename}\"\n",
    "    print(f\"Downloading {filename} to {destination}\")\n",
    "    url = hf_hub_url(model_id, filename)\n",
    "    Path(destination).parent.mkdir(exist_ok=True, parents=True)\n",
    "    urllib.request.urlretrieve(url, destination)\n",
    "    return destination\n",
    "\n",
    "processes = []\n",
    "with ThreadPoolExecutor(max_workers=len(filenames)) as executor:\n",
    "    for filename in filenames:\n",
    "        processes.append(executor.submit(download_file, filename))\n",
    "\n",
    "for task in as_completed(processes):\n",
    "    print(f\"Finished downloading {task.result()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89545c70-8e54-4265-aab7-e42e5fb606d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 14G\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Aug  4 04:41 .\n",
      "8.0K drwxr-xr-x 1 root root 4.0K Aug  4 04:41 ..\n",
      "4.0K -rw-r--r-- 1 root root 1.5K Aug  4 04:41 .gitattributes\n",
      " 12K -rw-r--r-- 1 root root 9.6K Aug  4 04:41 README.md\n",
      "4.0K -rw-r--r-- 1 root root  667 Aug  4 04:41 config.json\n",
      "4.0K -rw-r--r-- 1 root root 2.6K Aug  4 04:41 configuration_RW.py\n",
      "4.0K -rw-r--r-- 1 root root  111 Aug  4 04:41 generation_config.json\n",
      "4.0K -rw-r--r-- 1 root root 1.2K Aug  4 04:41 handler.py\n",
      " 48K -rw-r--r-- 1 root root  47K Aug  4 04:41 modelling_RW.py\n",
      "9.3G -rw-r--r-- 1 root root 9.3G Aug  4 04:43 pytorch_model-00001-of-00002.bin\n",
      "4.2G -rw-r--r-- 1 root root 4.2G Aug  4 04:42 pytorch_model-00002-of-00002.bin\n",
      " 20K -rw-r--r-- 1 root root  17K Aug  4 04:41 pytorch_model.bin.index.json\n",
      "4.0K -rw-r--r-- 1 root root  281 Aug  4 04:41 special_tokens_map.json\n",
      "2.7M -rw-r--r-- 1 root root 2.7M Aug  4 04:41 tokenizer.json\n",
      "4.0K -rw-r--r-- 1 root root  220 Aug  4 04:41 tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "! ls -lash /content/model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
