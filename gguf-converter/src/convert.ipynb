{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ead0b7e2",
   "metadata": {},
   "source": [
    "# Convert HuggingFace models to GGUF format\n",
    "\n",
    "This notebook converts HuggingFace models to GGUF format that's supported\n",
    "by llama.cpp. The notebook also supports downloading a model from HuggingFace\n",
    "directly by setting the `download_model_id` param in Substratus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d0dede",
   "metadata": {},
   "source": [
    "Load the params provided by substratus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11c4c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "params = {}\n",
    "params_path = Path(\"/content/params.json\")\n",
    "if params_path.is_file():\n",
    "    with params_path.open(\"r\", encoding=\"UTF-8\") as params_file:\n",
    "        params = json.load(params_file)\n",
    "if 'name' not in params:\n",
    "    raise Exception(\"Missing required param `name`\")\n",
    "\n",
    "name = params[\"name\"]\n",
    "\n",
    "output_path = params.get(\"output_path\", \"/content/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32938b1",
   "metadata": {},
   "source": [
    "Download the model from huggingFace if `download_model_id` params is set. Otherwise\n",
    "this expects HuggingFace model to be present at `/content/saved-model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bfe9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "model_path = \"/content/saved-model\"\n",
    "\n",
    "download_model_id = params.get(\"download_model_id\")\n",
    "if download_model_id:\n",
    "    model_path = \"/content/downloaded-model\"\n",
    "    snapshot_download(repo_id=download_model_id, local_dir=model_path,\n",
    "                      local_dir_use_symlinks=False, revision=\"main\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b44d82d",
   "metadata": {},
   "source": [
    "Convert the model to GGUF 16 bit so it can be further used with `llama.cpp/example/quantize` tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa56848-1d74-4c36-b34f-cda126fffd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# have to use this hack otherwise the python3 command won't work\n",
    "os.environ[\"MODEL_PATH\"] = model_path\n",
    "outfile = f\"{output_path}/{name}-f16.gguf\"\n",
    "os.environ[\"OUTFILE\"] = outfile\n",
    "\n",
    "! mkdir -p {output_path}\n",
    "! ls -lash {model_path}\n",
    "! python3 /content/llama.cpp/convert.py \\\n",
    "  --outfile $OUTFILE \\\n",
    "  --outtype f16 $MODEL_PATH\n",
    "\n",
    "! ls -lash {output_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ef3201-3508-4042-afae-a4a252b1f263",
   "metadata": {},
   "source": [
    "Upload the model if param `push_to_hub` was set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acda74c-56bb-462d-bc8d-a616a615eaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from pathlib import Path\n",
    "\n",
    "push_to_hub = params.get(\"push_to_hub\")\n",
    "if push_to_hub:\n",
    "    hf_api = HfApi()\n",
    "    model_id = push_to_hub\n",
    "    print(f\"Creating HuggingFace repo {model_id}\")\n",
    "    hf_api.create_repo(model_id, exist_ok=True, repo_type=\"model\")\n",
    "\n",
    "def push_to_huggingface(file):\n",
    "    hf_api.upload_file(\n",
    "        path_or_fileobj=file,\n",
    "        path_in_repo=Path(file).name,\n",
    "        repo_id=model_id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515857d2-0d49-4091-9d08-01ecc94dbdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "if push_to_hub:\n",
    "    push_to_huggingface(outfile)\n",
    "    readme_path = Path(model_path) / \"README.md\"\n",
    "    if readme_path.exists():\n",
    "        push_to_huggingface(readme_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce44075",
   "metadata": {},
   "source": [
    "Optionally create additional quantized models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d52a2b8-70cc-4d49-bf35-f4af88a45c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! quantize -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4fa933",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantize = params.get(\"quantize\")\n",
    "if quantize:\n",
    "    quantize = [q.strip() for q in quantize.split(\",\")]\n",
    "    for quantize_type in quantize:\n",
    "        filename = f\"{output_path}/{name}-{quantize_type}.gguf\"\n",
    "        os.environ[\"filename\"] = filename\n",
    "        os.environ[\"quantize_type\"] = quantize_type\n",
    "        print(f\"Running {quantize_type} quantization and writing to {filename}\")\n",
    "        ! quantize $OUTFILE $filename $quantize_type\n",
    "        if push_to_hub:\n",
    "            push_to_huggingface(filename)\n",
    "    ! ls -lash {output_path}"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
